{"version":3,"file":"groq-store.modern.js","sources":["../../src/listen.ts","../../src/drafts.ts","../../src/patch.ts","../../src/syncingDataset.ts","../../src/browser/getDocuments.ts","../../src/browser/index.ts","../../src/browser/support.ts","../../src/groqStore.ts"],"sourcesContent":["import {Subscription, MutationEvent, Config, ApiError} from './types'\n\nexport function listen(\n  EventSourceImpl: typeof EventSource,\n  config: Config,\n  handlers: {\n    open: () => void\n    error: (err: Error) => void\n    next: (event: MutationEvent) => void\n  }\n): Subscription {\n  const {projectId, dataset, token} = config\n  const headers = token ? {Authorization: `Bearer ${token}`} : undefined\n  const url = `https://${projectId}.api.sanity.io/v1/data/listen/${dataset}?query=*&effectFormat=mendoza`\n  const es = new EventSourceImpl(url, {withCredentials: true, headers} as any)\n\n  es.addEventListener('welcome', handlers.open, false)\n\n  es.addEventListener('mutation', getMutationParser(handlers.next), false)\n\n  es.addEventListener(\n    'channelError',\n    (msg: any) => {\n      es.close()\n\n      let data\n      try {\n        data = JSON.parse(msg.data) as ApiError\n      } catch (err) {\n        handlers.error(new Error('Unknown error parsing listener message'))\n        return\n      }\n\n      handlers.error(\n        new Error(data.message || data.error || `Listener returned HTTP ${data.statusCode}`)\n      )\n    },\n    false\n  )\n\n  es.addEventListener(\n    'error',\n    () => {\n      const origin = typeof window !== 'undefined' && window.location.origin\n      const hintSuffix = origin ? `, and that the CORS-origin (${origin}) is allowed` : ''\n      handlers.error(\n        new Error(\n          `Error establishing listener - check that the project ID and dataset are correct${hintSuffix}`\n        )\n      )\n    },\n    false\n  )\n\n  return {\n    unsubscribe: (): Promise<void> => Promise.resolve(es.close()),\n  }\n}\n\nfunction getMutationParser(cb: (event: MutationEvent) => void): (msg: any) => void {\n  return (msg: any) => {\n    let data\n    try {\n      data = JSON.parse(msg.data)\n    } catch (err) {\n      // intentional noop\n      return\n    }\n\n    cb(data)\n  }\n}\n","import {SanityDocument} from '@sanity/types'\n\nexport function isDraft(doc: SanityDocument): boolean {\n  return doc._id.startsWith('drafts.')\n}\n\nexport function getPublishedId(document: SanityDocument): string {\n  return isDraft(document) ? document._id.slice(7) : document._id\n}\n","import {SanityDocument} from '@sanity/types'\nimport {applyPatch} from 'mendoza'\n\nexport function applyPatchWithoutRev(\n  doc: SanityDocument | null,\n  patch: unknown[]\n): SanityDocument | null {\n  const patchDoc = {...doc} as Omit<SanityDocument, '_rev'>\n  delete patchDoc._rev\n  return applyPatch(patchDoc, patch)\n}\n","import {SanityDocument} from '@sanity/types'\nimport {listen} from './listen'\nimport {getPublishedId} from './drafts'\nimport {applyPatchWithoutRev} from './patch'\nimport {Config, EnvImplementations, MutationEvent, Subscription} from './types'\n\nconst DEBOUNCE_MS = 25\n\nfunction noop() {\n  return Promise.resolve()\n}\n\nexport function getSyncingDataset(\n  config: Config,\n  onNotifyUpdate: (docs: SanityDocument[]) => void,\n  {getDocuments, EventSource}: EnvImplementations\n): Subscription & {loaded: Promise<void>} {\n  const {projectId, dataset, listen: useListener, overlayDrafts, documentLimit} = config\n\n  if (!useListener) {\n    const loaded = getDocuments({projectId, dataset, documentLimit}).then(onUpdate).then(noop)\n    return {unsubscribe: noop, loaded}\n  }\n\n  const indexedDocuments = new Map<string, SanityDocument>()\n\n  // undefined until the listener has been set up and the initial export is done\n  let documents: SanityDocument[] | undefined\n\n  // holds any mutations that happen while fetching documents so they can be applied after updates\n  const buffer: MutationEvent[] = []\n\n  // Return a promise we can resolve once we've established a listener and reconciled any mutations\n  let onDoneLoading: () => void\n  let onLoadError: (error: Error) => void\n  const loaded = new Promise<void>((resolve, reject) => {\n    onDoneLoading = resolve\n    onLoadError = reject\n  })\n\n  // We don't want to flush updates while we're in the same transaction, so a normal\n  // throttle/debounce wouldn't do it. We need to wait and see if the next mutation is\n  // within the same transaction as the previous, and if not we can flush. Of course,\n  // we can't wait forever, so an upper threshold of X ms should be counted as \"ok to flush\"\n  let stagedDocs: SanityDocument[] | undefined\n  let previousTrx: string | undefined\n  let flushTimeout: number | undefined\n\n  const listener = listen(EventSource, config, {\n    next: onMutationReceived,\n    open: onOpen,\n    error: (error: Error) => onLoadError(error),\n  })\n\n  return {unsubscribe: listener.unsubscribe, loaded}\n\n  async function onOpen() {\n    const initial = await getDocuments({projectId, dataset, documentLimit})\n    documents = applyBufferedMutations(initial, buffer)\n    documents.forEach((doc) => indexedDocuments.set(doc._id, doc))\n    onUpdate(documents)\n    onDoneLoading()\n  }\n\n  function onMutationReceived(msg: MutationEvent) {\n    if (documents) {\n      applyMutation(msg)\n      scheduleUpdate(documents, msg)\n    } else {\n      buffer.push(msg)\n    }\n  }\n\n  function scheduleUpdate(docs: SanityDocument[], msg: MutationEvent) {\n    clearTimeout(flushTimeout)\n\n    if (previousTrx !== msg.transactionId && stagedDocs) {\n      // This is a new transaction, meaning we can immediately flush any pending\n      // doc updates if there are any\n      onUpdate(stagedDocs)\n      previousTrx = undefined\n    } else {\n      previousTrx = msg.transactionId\n      stagedDocs = docs.slice()\n    }\n\n    flushTimeout = setTimeout(onUpdate, DEBOUNCE_MS, docs.slice())\n  }\n\n  function onUpdate(docs: SanityDocument[]) {\n    stagedDocs = undefined\n    flushTimeout = undefined\n    previousTrx = undefined\n    onNotifyUpdate(overlayDrafts ? overlay(docs) : docs)\n  }\n\n  function applyMutation(msg: MutationEvent) {\n    if (!msg.effects || msg.documentId.startsWith('_.')) {\n      return\n    }\n\n    const document = indexedDocuments.get(msg.documentId) || null\n    replaceDocument(msg.documentId, applyPatchWithoutRev(document, msg.effects.apply))\n  }\n\n  function replaceDocument(id: string, document: SanityDocument | null) {\n    const current = indexedDocuments.get(id)\n    const docs = documents || []\n    const position = current ? docs.indexOf(current) : -1\n\n    if (position === -1 && document) {\n      // Didn't exist previously, but was now created. Add it.\n      docs.push(document)\n      indexedDocuments.set(id, document)\n    } else if (document) {\n      // Existed previously and still does. Replace it.\n      docs.splice(position, 1, document)\n      indexedDocuments.set(id, document)\n    } else {\n      // Existed previously, but is now deleted. Remove it.\n      docs.splice(position, 1)\n      indexedDocuments.delete(id)\n    }\n  }\n}\n\nfunction applyBufferedMutations(\n  documents: SanityDocument[],\n  mutations: MutationEvent[]\n): SanityDocument[] {\n  // Group by document ID\n  const groups = new Map<string, MutationEvent[]>()\n  mutations.forEach((mutation) => {\n    const group = groups.get(mutation.documentId) || []\n    group.push(mutation)\n    groups.set(mutation.documentId, group)\n  })\n\n  // Discard all mutations that happened before our current document\n  groups.forEach((group, id) => {\n    const document = documents.find((doc) => doc._id === id)\n    if (!document) {\n      // @todo handle\n      // eslint-disable-next-line no-console\n      console.warn('Received mutation for missing document %s', id)\n      return\n    }\n\n    // Mutations are sorted by timestamp, apply any that arrived after\n    // we fetched the initial documents\n    let hasFoundRevision = false\n    let current: SanityDocument | null = document\n    group.forEach((mutation) => {\n      hasFoundRevision = hasFoundRevision || mutation.previousRev === document._rev\n      if (!hasFoundRevision) {\n        return\n      }\n\n      if (mutation.effects) {\n        current = applyPatchWithoutRev(current, mutation.effects.apply)\n      }\n    })\n\n    // Replace the indexed documents\n    documents.splice(documents.indexOf(document), 1, current)\n  })\n\n  return documents\n}\n\nfunction overlay(documents: SanityDocument[]): SanityDocument[] {\n  const overlayed = new Map<string, SanityDocument>()\n\n  documents.forEach((doc) => {\n    const existing = overlayed.get(getPublishedId(doc))\n    if (doc._id.startsWith('drafts.')) {\n      // Drafts always overlay\n      overlayed.set(getPublishedId(doc), pretendThatItsPublished(doc))\n    } else if (!existing) {\n      // Published documents only override if draft doesn't exist\n      overlayed.set(doc._id, doc)\n    }\n  })\n\n  return Array.from(overlayed.values())\n}\n\n// Strictly speaking it would be better to allow groq-js to resolve `draft.<id>`,\n// but for now this will have to do\nfunction pretendThatItsPublished(doc: SanityDocument): SanityDocument {\n  return {...doc, _id: getPublishedId(doc)}\n}\n","import {SanityDocument} from '@sanity/types'\nimport {EnvImplementations} from '../types'\n\ntype StreamError = {error: {description?: string; type: string}}\ntype StreamResult = SanityDocument | StreamError\n\nexport const getDocuments: EnvImplementations['getDocuments'] = async function getDocuments({\n  projectId,\n  dataset,\n  token,\n  documentLimit,\n}: {\n  projectId: string\n  dataset: string\n  token?: string\n  documentLimit?: number\n}): Promise<SanityDocument[]> {\n  const url = `https://${projectId}.api.sanity.io/v1/data/export/${dataset}`\n  const headers = token ? {Authorization: `Bearer ${token}`} : undefined\n  const response = await fetch(url, {credentials: 'include', headers})\n\n  if (response.status !== 200) {\n    throw new Error(`Error streaming dataset: ${getError(await response.json())}`)\n  }\n\n  const stream = getDocumentStream(response.body)\n  const reader = stream.getReader()\n\n  const documents: SanityDocument[] = []\n  let result\n  let document\n  do {\n    result = await reader.read()\n    document = result.value\n\n    if (isStreamError(document)) {\n      throw new Error(`Error streaming dataset: ${document.error}`)\n    } else if (document && isRelevantDocument(document)) {\n      documents.push(document)\n    }\n\n    if (documentLimit && documents.length > documentLimit) {\n      reader.cancel('Reached document limit')\n      throw new Error(`Error streaming dataset: Reached limit of ${documentLimit} documents`)\n    }\n  } while (!result.done)\n\n  return documents\n}\n\nfunction getDocumentStream(body: Response['body']): ReadableStream<StreamResult> {\n  if (!body) {\n    throw new Error('Failed to read body from response')\n  }\n\n  let reader: ReadableStreamDefaultReader<Uint8Array> | undefined\n  let cancelled = false\n\n  function cancel() {\n    cancelled = true\n    if (reader) {\n      reader.cancel()\n    }\n  }\n\n  return new ReadableStream<SanityDocument>({\n    start(controller): void | PromiseLike<void> {\n      reader = body.getReader()\n      const decoder = new TextDecoder()\n      let buffer = ''\n\n      reader\n        .read()\n        .then(processResult)\n        .catch((err) => controller.error(err))\n\n      async function processResult(result: ReadableStreamReadResult<Uint8Array>): Promise<void> {\n        if (result.done) {\n          if (cancelled) {\n            return\n          }\n\n          buffer = buffer.trim()\n          if (buffer.length === 0) {\n            controller.close()\n            return\n          }\n\n          controller.enqueue(JSON.parse(buffer))\n          controller.close()\n          return\n        }\n\n        buffer += decoder.decode(result.value, {stream: true})\n        const lines = buffer.split('\\n')\n\n        for (let i = 0; i < lines.length - 1; ++i) {\n          const line = lines[i].trim()\n          if (line.length === 0) {\n            continue\n          }\n\n          try {\n            controller.enqueue(JSON.parse(line))\n          } catch (err) {\n            controller.error(err)\n            cancel()\n            return\n          }\n        }\n\n        buffer = lines[lines.length - 1]\n\n        if (!reader) {\n          return\n        }\n\n        try {\n          processResult(await reader.read())\n        } catch (err) {\n          controller.error(err)\n        }\n      }\n    },\n\n    cancel,\n  })\n}\n\nfunction isStreamError(result: StreamResult | undefined): result is StreamError {\n  if (!result) {\n    return false\n  }\n\n  if (!('error' in result) || typeof result.error !== 'object' || result.error === null) {\n    return false\n  }\n\n  return (\n    'description' in result.error &&\n    typeof (result as StreamError).error.description === 'string' &&\n    !('_id' in result)\n  )\n}\n\nfunction getError(body: any): string {\n  if (typeof body === 'object' && 'error' in body && 'message' in body) {\n    return body.message || body.error\n  }\n\n  return '<unknown error>'\n}\n\nfunction isRelevantDocument(doc: SanityDocument): boolean {\n  return !doc._id.startsWith('_.')\n}\n","import {groqStore as groqStoreApi} from '../groqStore'\nimport {Config, GroqStore} from '../types'\nimport {getDocuments} from './getDocuments'\nimport {assertEnvSupport} from './support'\n\nexport function groqStore(config: Config): GroqStore {\n  assertEnvSupport()\n\n  if (config.token) {\n    throw new Error('`token` option not currently supported in browser')\n  }\n\n  return groqStoreApi(config, {\n    EventSource: window.EventSource,\n    getDocuments,\n  })\n}\n\nexport {default as groq} from 'groq'\nexport {Subscription, GroqStore} from '../types'\n","export function assertEnvSupport(): void {\n  const required = ['EventSource', 'ReadableStream', 'fetch']\n  const unsupported = required.filter((api) => !(api in window))\n\n  if (unsupported.length > 0) {\n    throw new Error(`Browser not supported. Missing browser APIs: ${unsupported.join(', ')}`)\n  }\n}\n","import groq from 'groq'\nimport deepEqual from 'fast-deep-equal'\nimport {throttle} from 'throttle-debounce'\nimport {SanityDocument} from '@sanity/types'\nimport {parse, evaluate} from 'groq-js'\nimport {Config, EnvImplementations, GroqSubscription, GroqStore, Subscription} from './types'\nimport {getSyncingDataset} from './syncingDataset'\n\nexport function groqStore(config: Config, envImplementations: EnvImplementations): GroqStore {\n  let documents: SanityDocument[] = []\n  const executeThrottled = throttle(config.subscriptionThrottleMs || 50, executeAllSubscriptions)\n  const activeSubscriptions: GroqSubscription[] = []\n\n  let dataset: Subscription & {loaded: Promise<void>}\n\n  async function loadDataset() {\n    if (!dataset) {\n      dataset = getSyncingDataset(\n        config,\n        (docs) => {\n          documents = docs\n          executeThrottled()\n        },\n        envImplementations\n      )\n    }\n\n    await dataset.loaded\n  }\n\n  async function query<R = any>(groqQuery: string, params?: Record<string, unknown>): Promise<R> {\n    await loadDataset()\n    const tree = parse(groqQuery)\n    const result = await evaluate(tree, {dataset: documents, params})\n    return result.get()\n  }\n\n  async function getDocument(documentId: string): Promise<SanityDocument | null> {\n    await loadDataset()\n    return query(groq`*[_id == $id][0]`, {id: documentId})\n  }\n\n  async function getDocuments(documentIds: string[]): Promise<(SanityDocument | null)[]> {\n    await loadDataset()\n    const subQueries = documentIds.map((id) => `*[_id == \"${id}\"][0]`).join(',\\n')\n    return query(`[${subQueries}]`)\n  }\n\n  function subscribe<R = any>(\n    groqQuery: string,\n    params: Record<string, unknown>,\n    callback: (error: Error | undefined, result?: R) => void\n  ): Subscription {\n    if (!config.listen) {\n      throw new Error('Cannot use `subscribe()` without `listen: true`')\n    }\n\n    // @todo Execute the query against an empty dataset for validation purposes\n\n    // Store the subscription so we can re-run the query on new data\n    const subscription = {query: groqQuery, params, callback}\n    activeSubscriptions.push(subscription)\n\n    let unsubscribed = false\n    const unsubscribe = () => {\n      if (unsubscribed) {\n        return Promise.resolve()\n      }\n\n      unsubscribed = true\n      activeSubscriptions.splice(activeSubscriptions.indexOf(subscription), 1)\n      return Promise.resolve()\n    }\n\n    executeQuerySubscription(subscription)\n    return {unsubscribe}\n  }\n\n  function executeQuerySubscription(subscription: GroqSubscription) {\n    return query(subscription.query, subscription.params)\n      .then((res) => {\n        if ('previousResult' in subscription && deepEqual(subscription.previousResult, res)) {\n          return\n        }\n\n        subscription.previousResult = res\n        subscription.callback(undefined, res)\n      })\n      .catch((err) => {\n        subscription.callback(err)\n      })\n  }\n\n  function executeAllSubscriptions() {\n    activeSubscriptions.forEach(executeQuerySubscription)\n  }\n\n  function close() {\n    executeThrottled.cancel()\n    return dataset ? dataset.unsubscribe() : Promise.resolve()\n  }\n\n  return {query, getDocument, getDocuments, subscribe, close}\n}\n"],"names":["listen","EventSourceImpl","config","handlers","projectId","dataset","token","es","withCredentials","headers","Authorization","undefined","cb","addEventListener","open","next","msg","data","JSON","parse","err","close","error","Error","message","statusCode","origin","window","location","unsubscribe","Promise","resolve","getPublishedId","document","_id","startsWith","slice","applyPatchWithoutRev","doc","patch","patchDoc","_rev","applyPatch","noop","getDocuments","async","documentLimit","url","response","fetch","credentials","status","body","json","reader","cancelled","cancel","ReadableStream","start","controller","getReader","decoder","TextDecoder","buffer","read","then","processResult","result","done","trim","length","enqueue","decode","value","stream","lines","split","i","line","catch","getDocumentStream","documents","isStreamError","push","description","groqStore","unsupported","filter","api","join","assertEnvSupport","envImplementations","executeThrottled","throttle","subscriptionThrottleMs","activeSubscriptions","forEach","executeQuerySubscription","loadDataset","onNotifyUpdate","EventSource","useListener","overlayDrafts","loaded","onUpdate","indexedDocuments","Map","onDoneLoading","onLoadError","reject","stagedDocs","previousTrx","flushTimeout","effects","documentId","get","id","current","docs","position","indexOf","set","splice","delete","replaceDocument","apply","applyMutation","clearTimeout","transactionId","setTimeout","scheduleUpdate","initial","mutations","groups","mutation","group","find","console","warn","hasFoundRevision","previousRev","applyBufferedMutations","overlayed","existing","pretendThatItsPublished","Array","from","values","overlay","getSyncingDataset","query","groqQuery","params","tree","evaluate","subscription","res","deepEqual","previousResult","callback","getDocument","groq","documentIds","map","subscribe","unsubscribed","groqStoreApi"],"mappings":"6aAEgBA,EACdC,EACAC,EACAC,GAMA,MAAMC,UAACA,EAADC,QAAYA,EAAZC,MAAqBA,GAASJ,EAG9BK,EAAK,IAAIN,aADQG,kCAA0CC,iCAC7B,CAACG,iBAAiB,EAAMC,QAF5CH,EAAQ,CAACI,wBAAyBJ,QAAWK,IA+C/D,IAA2BC,EALzB,OAtCAL,EAAGM,iBAAiB,UAAWV,EAASW,MAAM,GAE9CP,EAAGM,iBAAiB,YAyCKD,EAzCyBT,EAASY,KA0CnDC,IACN,IAAIC,EACJ,IACEA,EAAOC,KAAKC,MAAMH,EAAIC,MACtB,MAAOG,GAEP,OAGFR,EAAGK,MAnD6D,GAElEV,EAAGM,iBACD,eACCG,IAGC,IAAIC,EAFJV,EAAGc,QAGH,IACEJ,EAAOC,KAAKC,MAAMH,EAAIC,MACtB,MAAOG,GAEP,YADAjB,EAASmB,MAAM,IAAIC,MAAM,2CAI3BpB,EAASmB,MACP,IAAIC,MAAMN,EAAKO,SAAWP,EAAKK,iCAAmCL,EAAKQ,eAG3E,GAGFlB,EAAGM,iBACD,QACA,KACE,MAAMa,EAA2B,oBAAXC,QAA0BA,OAAOC,SAASF,OAEhEvB,EAASmB,MACP,IAAIC,yFAFaG,iCAAwCA,gBAAuB,QAOpF,GAGK,CACLG,YAAa,IAAqBC,QAAQC,QAAQxB,EAAGc,mBCjDzCW,EAAeC,GAC7B,OAAeA,EAJJC,IAAIC,WAAW,WAICF,EAASC,IAAIE,MAAM,GAAKH,EAASC,aCJ9CG,EACdC,EACAC,GAEA,MAAMC,OAAeF,GAErB,cADOE,EAASC,KACTC,EAAWF,EAAUD,GCD9B,SAASI,IACP,OAAOb,QAAQC,6BCHJa,EAAmDC,gBAA4BzC,UAC1FA,EAD0FC,QAE1FA,EAF0FC,MAG1FA,EAH0FwC,cAI1FA,IAOA,MAAMC,aAAiB3C,kCAA0CC,IAC3DI,EAAUH,EAAQ,CAACI,wBAAyBJ,QAAWK,EACvDqC,QAAiBC,MAAMF,EAAK,CAACG,YAAa,UAAWzC,QAAAA,IAE3D,GAAwB,MAApBuC,EAASG,OACX,UAAU5B,mCA4HQ,iBADJ6B,QA3H6CJ,EAASK,SA4HtC,UAAWD,GAAQ,YAAaA,EACvDA,EAAK5B,SAAW4B,EAAK9B,MAGvB,oBALT,IAAkB8B,EAxHhB,MACME,EAwBR,SAA2BF,GACzB,IAAKA,EACH,UAAU7B,MAAM,qCAGlB,IAAI+B,EACAC,GAAY,EAEhB,SAASC,IACPD,GAAY,EACRD,GACFA,EAAOE,SAIX,WAAWC,eAA+B,CACxCC,MAAMC,GACJL,EAASF,EAAKQ,YACd,MAAMC,EAAU,IAAIC,YACpB,IAAIC,EAAS,GAEbT,EACGU,OACAC,KAGHpB,eAAeqB,EAAcC,GAC3B,GAAIA,EAAOC,KAAM,CACf,GAAIb,EACF,OAIF,OADAQ,EAASA,EAAOM,OACM,IAAlBN,EAAOO,QAKXX,EAAWY,QAAQrD,KAAKC,MAAM4C,SAJ5BJ,EAAWtC,QASf0C,GAAUF,EAAQW,OAAOL,EAAOM,MAAO,CAACC,QAAQ,IAChD,MAAMC,EAAQZ,EAAOa,MAAM,MAE3B,IAAK,IAAIC,EAAI,EAAGA,EAAIF,EAAML,OAAS,IAAKO,EAAG,CACzC,MAAMC,EAAOH,EAAME,GAAGR,OACtB,GAAoB,IAAhBS,EAAKR,OAIT,IACEX,EAAWY,QAAQrD,KAAKC,MAAM2D,IAC9B,MAAO1D,GAGP,OAFAuC,EAAWrC,MAAMF,QACjBoC,KAOJ,GAFAO,EAASY,EAAMA,EAAML,OAAS,GAEzBhB,EAIL,IACEY,QAAoBZ,EAAOU,QAC3B,MAAO5C,GACPuC,EAAWrC,MAAMF,MA9ClB2D,MAAO3D,GAAQuC,EAAWrC,MAAMF,KAmDrCoC,OAAAA,IApGawB,CAAkBhC,EAASI,MACpBQ,YAEhBqB,EAA8B,GACpC,IAAId,EACAlC,EACJ,EAAG,CAID,GAHAkC,QAAeb,EAAOU,OACtB/B,EAAWkC,EAAOM,MAEdS,EAAcjD,GAChB,UAAUV,kCAAkCU,EAASX,OAKvD,GAJWW,IAA+BA,EAqHhCC,IAAIC,WAAW,OApHvB8C,EAAUE,KAAKlD,GAGba,GAAiBmC,EAAUX,OAASxB,EAEtC,MADAQ,EAAOE,OAAO,8BACJjC,mDAAmDuB,sBAEvDqB,EAAOC,MAEjB,OAAOa,GAkFT,SAASC,EAAcf,GACrB,QAAKA,GAIC,UAAWA,GAAmC,iBAAjBA,EAAO7C,OAAuC,OAAjB6C,EAAO7C,OAKrE,gBAAiB6C,EAAO7C,OAC6B,iBAA7C6C,EAAuB7C,MAAM8D,eACnC,QAASjB,YCxICkB,EAAUnF,GAGxB,cCPA,MACMoF,EADW,CAAC,cAAe,iBAAkB,SACtBC,OAAQC,KAAUA,KAAO7D,SAEtD,GAAI2D,EAAYhB,OAAS,EACvB,UAAU/C,sDAAsD+D,EAAYG,KAAK,ODCnFC,GAEIxF,EAAOI,MACT,UAAUiB,MAAM,qDAGlB,gBEJwBrB,EAAgByF,GACxC,IAAIV,EAA8B,GAClC,MAAMW,EAAmBC,EAAS3F,EAAO4F,wBAA0B,GAmFnE,WACEC,EAAoBC,QAAQC,KAnFxBF,EAA0C,GAEhD,IAAI1F,EAEJwC,eAAeqD,IACR7F,IACHA,WJJJH,EACAiG,GACAvD,aAACA,EAADwD,YAAeA,IAEf,MAAMhG,UAACA,EAADC,QAAYA,EAASL,OAAQqG,EAA7BC,cAA0CA,EAA1CxD,cAAyDA,GAAiB5C,EAEhF,IAAKmG,EAEH,MAAO,CAACxE,YAAac,EAAM4D,OADZ3D,EAAa,CAACxC,UAAAA,EAAWC,QAAAA,EAASyC,cAAAA,IAAgBmB,KAAKuC,GAAUvC,KAAKtB,IAIvF,MAAM8D,EAAmB,IAAIC,IAG7B,IAAIzB,EAGJ,MAAMlB,EAA0B,GAGhC,IAAI4C,EACAC,EACJ,MAAML,EAAS,IAAIzE,QAAc,CAACC,EAAS8E,KACzCF,EAAgB5E,EAChB6E,EAAcC,IAOhB,IAAIC,EACAC,EACAC,EAQJ,MAAO,CAACnF,YANS7B,EAAOoG,EAAalG,EAAQ,CAC3Ca,KAeF,SAA4BC,GACtBiE,GA+BN,SAAuBjE,GACrB,IAAKA,EAAIiG,SAAWjG,EAAIkG,WAAW/E,WAAW,MAC5C,OAGF,MAAMF,EAAWwE,EAAiBU,IAAInG,EAAIkG,aAAe,MAI3D,SAAyBE,EAAYnF,GACnC,MAAMoF,EAAUZ,EAAiBU,IAAIC,GAC/BE,EAAOrC,GAAa,GACpBsC,EAAWF,EAAUC,EAAKE,QAAQH,IAAY,GAElC,IAAdE,GAAmBtF,GAErBqF,EAAKnC,KAAKlD,GACVwE,EAAiBgB,IAAIL,EAAInF,IAChBA,GAETqF,EAAKI,OAAOH,EAAU,EAAGtF,GACzBwE,EAAiBgB,IAAIL,EAAInF,KAGzBqF,EAAKI,OAAOH,EAAU,GACtBd,EAAiBkB,OAAOP,IAnB1BQ,CAAgB5G,EAAIkG,WAAY7E,EAAqBJ,EAAUjB,EAAIiG,QAAQY,QApCzEC,CAAc9G,GAOlB,SAAwBsG,EAAwBtG,GAC9C+G,aAAaf,GAETD,IAAgB/F,EAAIgH,eAAiBlB,GAGvCN,EAASM,GACTC,OAAcpG,IAEdoG,EAAc/F,EAAIgH,cAClBlB,EAAaQ,EAAKlF,SAGpB4E,EAAeiB,WAAWzB,EAhFV,GAgFiCc,EAAKlF,SAnBpD8F,CAAejD,EAAWjE,IAE1B+C,EAAOoB,KAAKnE,IAnBdF,KAMF+B,iBACE,MAAMsF,QAAgBvF,EAAa,CAACxC,UAAAA,EAAWC,QAAAA,EAASyC,cAAAA,IACxDmC,EAoEJ,SACEA,EACAmD,GAGA,MAAMC,EAAS,IAAI3B,IAoCnB,OAnCA0B,EAAUpC,QAASsC,IACjB,MAAMC,EAAQF,EAAOlB,IAAImB,EAASpB,aAAe,GACjDqB,EAAMpD,KAAKmD,GACXD,EAAOZ,IAAIa,EAASpB,WAAYqB,KAIlCF,EAAOrC,QAAQ,CAACuC,EAAOnB,KACrB,MAAMnF,EAAWgD,EAAUuD,KAAMlG,GAAQA,EAAIJ,MAAQkF,GACrD,IAAKnF,EAIH,YADAwG,QAAQC,KAAK,4CAA6CtB,GAM5D,IAAIuB,GAAmB,EACnBtB,EAAiCpF,EACrCsG,EAAMvC,QAASsC,IACbK,EAAmBA,GAAoBL,EAASM,cAAgB3G,EAASQ,KACpEkG,GAIDL,EAASrB,UACXI,EAAUhF,EAAqBgF,EAASiB,EAASrB,QAAQY,UAK7D5C,EAAUyC,OAAOzC,EAAUuC,QAAQvF,GAAW,EAAGoF,KAG5CpC,EA7GO4D,CAAuBV,EAASpE,GAC5CkB,EAAUe,QAAS1D,GAAQmE,EAAiBgB,IAAInF,EAAIJ,IAAKI,IACzDkE,EAASvB,GACT0B,KAVArF,MAAQA,GAAiBsF,EAAYtF,KAGTO,YAAa0E,OAAAA,GAmC3C,SAASC,EAASc,GAChBR,OAAanG,EACbqG,OAAerG,EACfoG,OAAcpG,EACdwF,EAAeG,EA6EnB,SAAiBrB,GACf,MAAM6D,EAAY,IAAIpC,IAatB,OAXAzB,EAAUe,QAAS1D,IACjB,MAAMyG,EAAWD,EAAU3B,IAAInF,EAAeM,IAC1CA,EAAIJ,IAAIC,WAAW,WAErB2G,EAAUrB,IAAIzF,EAAeM,GAYnC,SAAiCA,GAC/B,YAAWA,GAAKJ,IAAKF,EAAeM,KAbG0G,CAAwB1G,IACjDyG,GAEVD,EAAUrB,IAAInF,EAAIJ,IAAKI,KAIpB2G,MAAMC,KAAKJ,EAAUK,UA3FKC,CAAQ9B,GAAQA,II5EnC+B,CACRnJ,EACCoH,IACCrC,EAAYqC,EACZ1B,KAEFD,UAIEtF,EAAQkG,OAGhB1D,eAAeyG,EAAeC,EAAmBC,SACzCtD,IACN,MAAMuD,EAAOtI,EAAMoI,GAEnB,aADqBG,EAASD,EAAM,CAACpJ,QAAS4E,EAAWuE,OAAAA,KAC3CrC,MA4ChB,SAASlB,EAAyB0D,GAChC,OAAOL,EAAMK,EAAaL,MAAOK,EAAaH,QAC3CvF,KAAM2F,IACD,mBAAoBD,GAAgBE,EAAUF,EAAaG,eAAgBF,KAI/ED,EAAaG,eAAiBF,EAC9BD,EAAaI,cAASpJ,EAAWiJ,MAElC7E,MAAO3D,IACNuI,EAAaI,SAAS3I,KAa5B,MAAO,CAACkI,MAAAA,EAAOU,YAjEfnH,eAA2BqE,GAEzB,aADMhB,IACCoD,EAAMW,8BAAwB,CAAC7C,GAAIF,KA+DhBtE,aA5D5BC,eAA4BqH,GAG1B,aAFMhE,IAECoD,MADYY,EAAYC,IAAK/C,gBAAoBA,UAAW3B,KAAK,YA0DhC2E,UAtD1C,SACEb,EACAC,EACAO,GAEA,IAAK7J,EAAOF,OACV,UAAUuB,MAAM,mDAMlB,MAAMoI,EAAe,CAACL,MAAOC,EAAWC,OAAAA,EAAQO,SAAAA,GAChDhE,EAAoBZ,KAAKwE,GAEzB,IAAIU,GAAe,EAYnB,OADApE,EAAyB0D,GAClB,CAAC9H,YAXY,KACdwI,IAIJA,GAAe,EACftE,EAAoB2B,OAAO3B,EAAoByB,QAAQmC,GAAe,IAJ7D7H,QAAQC,aAoCgCV,MALrD,WAEE,OADAuE,EAAiBpC,SACVnD,EAAUA,EAAQwB,cAAgBC,QAAQC,YFvF5CuI,CAAapK,EAAQ,CAC1BkG,YAAazE,OAAOyE,YACpBxD,aAAAA"}